#############################################################################
##############################################################################
#
#	DO NOT EDIT - file is being maintained by puppet
#
##############################################################################
##############################################################################


START   = FALSE
SUSPEND = FALSE
PREEMPT = FALSE
KILL    = FALSE

DAEMON_LIST = MASTER, SCHEDD


# define max interval for evaluating periodic release/removal
MAX_PERIODIC_EXPR_INTERVAL = 1200

# hold jobs using absurd amounts of disk (100+ GB) or using more memory than requested.
SYSTEM_PERIODIC_HOLD = \
   (JobStatus == 1 || JobStatus == 2) && ((DiskUsage > 100000000 || ResidentSetSize > 1000*RequestMemory))

# Report why the stupid thing went on hold.
SYSTEM_PERIODIC_HOLD_REASON = strcat("Job in status ", JobStatus, " put on hold by SYSTEM_PERIODIC_HOLD due to ", ifThenElse(isUndefined(DiskUsage) || DiskUsage < 100000000, strcat("memory usage ", ResidentSetSize), strcat("disk usage ", DiskUsage)), ".")

# forceful removal of running after 3 days, held jobs after 6 hours,
# and anything trying to run more than 10 times
SYSTEM_PERIODIC_REMOVE = \
   (JobStatus == 5 && CurrentTime - EnteredCurrentStatus > 3600*6) || \
   (JobStatus == 2 && CurrentTime - EnteredCurrentStatus > 3600*24*4) || \
   (JobStatus == 5 && JobRunCount >= 10) || \
   (JobStatus == 5 && HoldReasonCode =?= 14 && HoldReasonSubCode =?= 2)

# Record why the job was removed
SYSTEM_PERIODIC_REMOVE_REASON = strcat("Job removed by SYSTEM_PERIODIC_REMOVE due to ", \
   ifThenElse((JobStatus == 5 && CurrentTime - EnteredCurrentStatus > 3600*6), "being in hold state for 6 hours", \
   ifThenElse((JobStatus == 2 && CurrentTime - EnteredCurrentStatus > 3600*24*4), "runtime of longer than 96 hours", \
   ifThenElse((JobStatus == 5 && JobRunCount >= 10), "more than 10 restarts", \
              "input files missing"))), ".")

# Release from hold a few times
# HoldReasonCode 13 and 14 have to do with failures to upload
SYSTEM_PERIODIC_RELEASE = \
  (   (HoldReasonCode =?= 12) \
   || (HoldReasonCode =?= 13) \
   || (HoldReasonCode =?= 14 && HoldReasonSubCode =!= 2)) \
   && ((CurrentTime - EnteredCurrentStatus > 5*60) \
   && (JobRunCount < 10))

EVENT_LOG                          = $(LOG)/EventLog
EVENT_LOG_USE_XML                  = True
EVENT_LOG_JOB_AD_INFORMATION_ATTRS = Owner,CurrentHosts,x509userproxysubject,AccountingGroup,GlobalJobId,QDate,JobStartDate,JobCurrentStartDate,JobFinishedHookDone,RemoteHost

##  Maximum number of simultaneous downloads of output files from
##  execute machines to the submit machine (limit applied per schedd).
##  The value 0 means unlimited.
MAX_CONCURRENT_DOWNLOADS = 50

##  Maximum number of simultaneous uploads of input files from the
##  submit machine to execute machines (limit applied per schedd).
##  The value 0 means unlimited.
MAX_CONCURRENT_UPLOADS = 50

# Next line is commented out because gratia RPM now sets this value too.
# PER_JOB_HISTORY_DIR = /var/lib/gratia/data

MAX_HISTORY_LOG = 104857600
MAX_HISTORY_ROTATIONS = 10

## Configure integration with QPID
QMF_PUBLISH_SUBMISSIONS = FALSE
QMF_BROKER_USERNAME = submitter
QMF_BROKER_PASSWORD_FILE = /etc/condor/qpid_passfile_submitter
QMF_BROKER_AUTH_MECH = PLAIN
QMF_BROKER_HOST = red-web.unl.edu
QMF_BROKER_PORT = 5672
ENABLE_RUNTIME_CONFIG = TRUE

USE_CLONE_TO_CREATE_PROCESSES = false

# Settings for sleeper pool schedd
SLEEPERSCHEDD = $(SCHEDD)
SLEEPERSCHEDD_ARGS = -f -local-name sleeper
SLEEPERSCHEDD_SPOOL = $(SPOOL)/sleeper
SCHEDD.SLEEPER.COLLECTOR_HOST = $(CONDOR_HOST):9619
SCHEDD.SLEEPER.SCHEDD_NAME = sleeper@$(FULL_HOSTNAME)
SCHEDD.SLEEPER.SPOOL = $(SLEEPERSCHEDD_SPOOL)
SCHEDD.SLEEPER.JOB_QUEUE_LOG = $(SLEEPERSCHEDD_SPOOL)/job_queue.log
SCHEDD.SLEEPER.SCHEDD_LOG = $(LOG)/sleeper/ScheddLog
SCHEDD.SLEEPER.SCHEDD_ADDRESS_FILE = $(SLEEPERSCHEDD_SPOOL)/.schedd_address
SCHEDD.SLEEPER.SCHEDD_DAEMON_AD_FILE = $(SLEEPERSCHEDD_SPOOL)/.schedd_classad
SCHEDD.SLEEPER.HISTORY = $(SLEEPERSCHEDD_SPOOL)/history
SCHEDD.SLEEPER.QMF_STOREFILE = $(LOG)/sleeper/.schedd_storefile
SCHEDD.SLEEPER.FLOCK_TO = red-condor.unl.edu:9619
SCHEDD.SLEEPER.FLOCK_NEGOTIATOR_HOSTS = red-condor.unl.edu:9619
SCHEDD.SLEEPER.FLOCK_COLLECTOR_HOSTS = red-condor.unl.edu:9619
SCHEDD.SLEEPER.QUEUE_SUPER_USER_MAY_IMPERSONATE = .*
VALID_SPOOL_FILES = $(VALID_SPOOL_FILES), sleeper
DAEMON_LIST = $(DAEMON_LIST), SLEEPERSCHEDD

#
# JobRouter, thy name is legion...
#

DAEMON_LIST = $(DAEMON_LIST) JOB_ROUTER

JOB_ROUTER_ENTRIES = \
   [ \
     Requirements = target.JobUniverse == 5 && target.HasLocalCustomizations =!= true; \
     EditJobInPlace = true; \
     /* JobRouter chokes if this is not set, although it's obviously meaningless. */ \
     GridResource = "condor local local"; \
     eval_set_IsT3User = regexp("cms.other.user.t3.*", ifThenElse(isUndefined(AccountingGroup), "", AccountingGroup)); \
     /* Append extra requirements. */ \
     copy_Requirements = "OriginalRequirements"; \
     set_Requirements = (my.OriginalRequirements) && (my.HasLocalCustomizations =?= true) && (target.IS_GLIDEIN=!=TRUE || target.GLIDECLIENT_Group=?="T2Overflow"); \
     set_HasLocalCustomizations = true; \
     set_WantIOProxy = true; \
     set_RequestedChroot = "SL5"; \
     /* Some ClassAds tap-dancing to do minimum requests. */ \
     copy_RequestCpus = "OriginalRequestCpus"; \
     eval_set_RequestCpus = ifThenElse(isUndefined(OriginalRequestCpus), 1, OriginalRequestCpus); \
     copy_RequestDisk = "OriginalRequestDisk"; \
     eval_set_RequestDisk = ifThenElse(OriginalRequestDisk > 10240, OriginalRequestDisk, 10240); \
     copy_RequestMemory = "OriginalRequestMemory"; \
     eval_set_RequestMemory = ifThenElse(regexp("cms\.", ifThenElse(isUndefined(AccountingGroup), "", AccountingGroup)) && OriginalRequestMemory < 2500, 2500, \
                              ifThenElse(OriginalRequestMemory < 1900, 1900, OriginalRequestMemory)); \
     set_IsT2Overflow = ((CurrentTime - QDate) > 60 * 60 * 24) && (IsT3User == false); \
     name = "Local_Transform"; \
   ]

JOB_ROUTER_DEFAULTS =  []

JOB_ROUTER_SOURCE_JOB_CONSTRAINT = true

